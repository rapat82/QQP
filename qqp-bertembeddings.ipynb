{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we use the same siamese LSTM method as described in \n",
    "\n",
    "https://github.com/rapat82/QQP/blob/master/QQP_LSTM.ipynb\n",
    "\n",
    "# instead of training embeddings we extract embeddings from BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{blue}{\\text{Summary of main results}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Embeddings from BERT model can enhance performance when coupled with non linear correlators (see the link above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Obtaining sentence embeddings by just using the [CLS] token output from the last BERT layer typically does not constitute a good sentence representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Below BERT model is only used in evaluation mode bert_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Fine tuning a BERT model along the lines of \n",
    "https://github.com/rapat82/ReOrNot/blob/master/realornot-bertbase.ipynb\n",
    "\n",
    "# would give much better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Since this dataset is quite large, training BERT for this task could take a long time on a GPU (upto 25-30 hrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - To run this notebook locally, change the path of data files and files for pretrained bert model accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-cased-vocab.txt\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-multilingual-cased-vocab.txt\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-large-uncased-vocab.txt\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-large-cased-vocab.txt\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-multilingual-uncased-vocab.txt\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-chinese-vocab.txt\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-large-uncased/bert_config.json\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-large-uncased/pytorch_model.bin\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-uncased/pytorch_model.bin\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-cased/bert_config.json\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-cased/pytorch_model.bin\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-multilingual-cased/bert_config.json\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-multilingual-cased/pytorch_model.bin\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-multilingual-uncased/bert_config.json\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-multilingual-uncased/pytorch_model.bin\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-chinese/bert_config.json\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-base-chinese/pytorch_model.bin\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-large-cased/bert_config.json\n",
      "/kaggle/input/pretrained-bert-models-for-pytorch/bert-large-cased/pytorch_model.bin\n",
      "/kaggle/input/quora-question-pairs/train.csv.zip\n",
      "/kaggle/input/quora-question-pairs/test.csv.zip\n",
      "/kaggle/input/quora-question-pairs/sample_submission.csv.zip\n",
      "/kaggle/input/quora-question-pairs/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df_data = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = train_df_data['question1'].values\n",
    "q2 = train_df_data['question2'].values\n",
    "y = train_df_data['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt',\n",
    "                                         do_lower_case=True)\n",
    "def numericalize(q1_list, m_len):\n",
    "    output_list = []\n",
    "    output_list_token_type_ids = []\n",
    "    output_list_attention_mask = []\n",
    "    for i in range(len(q1_list)):\n",
    "        temp = tokenizer.encode_plus(q1_list[i], max_length=m_len, \n",
    "                                            truncation_strategy='longest_first', \n",
    "                                            pad_to_max_length=True, return_tensors='pt')\n",
    "        output_list.append(temp['input_ids'])\n",
    "        output_list_token_type_ids.append(temp['token_type_ids'])\n",
    "        output_list_attention_mask.append(temp['attention_mask'])\n",
    "    output_tensor = torch.stack(output_list).squeeze()\n",
    "    output_tensor_ids = torch.stack(output_list_token_type_ids).squeeze()\n",
    "    output_tensor_mask = torch.stack(output_list_attention_mask).squeeze()\n",
    "    return output_tensor, output_tensor_ids, output_tensor_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_len = 100\n",
    "q1_tokens, q1_ids, q1_masks = numericalize(q1, m_len)\n",
    "q2_tokens, q2_ids, q2_masks = numericalize(q2, m_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=33\n",
    "np.random.seed(seed)\n",
    "perm = np.random.permutation(q1_tokens.shape[0])\n",
    "q1t_s = np.zeros_like(q1_tokens)\n",
    "q1id_s = np.zeros_like(q1_ids)\n",
    "q1m_s = np.zeros_like(q1_masks)\n",
    "q2t_s = np.zeros_like(q2_tokens)\n",
    "q2id_s = np.zeros_like(q2_ids)\n",
    "q2m_s = np.zeros_like(q2_masks)\n",
    "y_s = np.zeros_like(y)\n",
    "np.take(q1_tokens,perm,axis=0,out=q1t_s)\n",
    "np.take(q1_ids,perm,axis=0,out=q1id_s)\n",
    "np.take(q1_masks,perm,axis=0,out=q1m_s)\n",
    "np.take(q2_tokens,perm,axis=0,out=q2t_s)\n",
    "np.take(q2_ids,perm,axis=0,out=q2id_s)\n",
    "np.take(q2_masks,perm,axis=0,out=q2m_s)\n",
    "np.take(y,perm,axis=0,out=y_s)\n",
    "split_frac=0.9\n",
    "iindex = int(len(q1_tokens)*split_frac)\n",
    "q1_tok_train, q1_ids_train, q1_masks_train = q1t_s[:iindex], q1id_s[:iindex], q1m_s[:iindex]\n",
    "q1_tok_val, q1_ids_val, q1_masks_val = q1t_s[iindex:], q1id_s[iindex:], q1m_s[iindex:]\n",
    "q2_tok_train, q2_ids_train, q2_masks_train = q2t_s[:iindex], q2id_s[:iindex], q2m_s[:iindex]\n",
    "q2_tok_val, q2_ids_val, q2_masks_val = q2t_s[iindex:], q2id_s[iindex:], q2m_s[iindex:]\n",
    "train_y, val_y = y_s[:iindex], y_s[iindex:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(q1_tok_train), torch.from_numpy(q1_ids_train), \n",
    "                         torch.from_numpy(q1_masks_train), torch.from_numpy(q2_tok_train), \n",
    "                         torch.from_numpy(q2_ids_train), torch.from_numpy(q2_masks_train), \n",
    "                         torch.from_numpy(train_y))\n",
    "val_data = TensorDataset(torch.from_numpy(q1_tok_val), torch.from_numpy(q1_ids_val), \n",
    "                         torch.from_numpy(q1_masks_val), torch.from_numpy(q2_tok_val), \n",
    "                         torch.from_numpy(q2_ids_val), torch.from_numpy(q2_masks_val), \n",
    "                         torch.from_numpy(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs = 128\n",
    "train_loader = DataLoader(train_data, shuffle = True, batch_size=train_bs)\n",
    "valid_loader = DataLoader(val_data, shuffle = True, batch_size=train_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu=torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "#bert_config.output_hidden_states=True\n",
    "bert_model = BertModel.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config = bert_config)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class QQPLSTM(nn.Module):\n",
    "    # The model below will calculate the similarity of two questions\n",
    "    #def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim,\n",
    "    #            n_layers, drop_prob = 0.5):\n",
    "    def __init__(self, seq_len, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \n",
    "        super(QQPLSTM, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim =embedding_dim\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           n_layers, dropout = drop_prob,\n",
    "                           batch_first = True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(7*hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward_half(self, x, hidden):\n",
    "        lstm_output, hidden = self.lstm(x, hidden) \n",
    "        lstm_output = lstm_output.contiguous().view(-1,self.hidden_dim)\n",
    "        \n",
    "        return lstm_output, hidden\n",
    "    def forward(self, x1, x2, hidden1, hidden2):\n",
    "        batch_size = x1.size(0)\n",
    "        lstm_output1, hidden1 = self.forward_half(x1, hidden1)\n",
    "        lstm_output2, hidden2 = self.forward_half(x2, hidden2)\n",
    "        lstm_output = torch.cat((torch.max(lstm_output1,lstm_output2), \n",
    "                                 torch.abs(lstm_output1-lstm_output2),\n",
    "                                 lstm_output1*lstm_output2,\n",
    "                                 torch.abs(lstm_output1**2-lstm_output2**2),\n",
    "                                torch.abs(lstm_output1**3-lstm_output2**3),\n",
    "                                torch.abs(lstm_output1**4-lstm_output2**4),\n",
    "                                torch.abs(lstm_output1**3-lstm_output2**3)*(lstm_output1+lstm_output2)/2), \n",
    "                                dim=1)\n",
    "        out = self.dropout(lstm_output)\n",
    "        out = self.fc(out)\n",
    "        #out = self.dropout2(out)\n",
    "        #out = self.fc2(out)\n",
    "        #out = self.dropout3(out)\n",
    "        #out = self.fc3(out)\n",
    "        # out is now of dimension rows X output_size\n",
    "        sigmoid_out = self.sigmoid(out)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)\n",
    "        # sigmoid_out is now of dimension batch_size X columns\n",
    "        sigmoid_out = sigmoid_out[:,-1]\n",
    "        # This was the step where we took the last batch of 'labels'\n",
    "        return sigmoid_out, hidden1, hidden2\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        # Let's create new tensors initialized to zero for the hidden state and cell state of the LSTM\n",
    "        # these should be two tensors of size n_layers X batch_size X hidden\n",
    "        # There should be n_layers*2 for bidirectional \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden=(weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden=(weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 1\n",
    "embedding_dim = 768\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "seq_len = 200\n",
    "net = QQPLSTM(seq_len, output_size, embedding_dim, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_actual, y_pred):\n",
    "#    y_ = y_pred > 0\n",
    "#    return np.sum(y_actual == y_).astype('int') / y_actual.shape[0]\n",
    "    y_ = np.round(np.array(y_pred))\n",
    "    return np.sum(y_actual == y_) / y_actual.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  2 22:56:51 2020\n",
      "Epoch: 1/6---finished---accuracy:0.7389140706654279\n",
      "Mon Mar  2 23:44:11 2020\n",
      "Epoch: 2/6---finished---accuracy:0.7756160815381489\n",
      "Tue Mar  3 00:31:33 2020\n",
      "Epoch: 3/6---finished---accuracy:0.7797341790297294\n",
      "Tue Mar  3 01:18:53 2020\n",
      "Epoch: 4/6---finished---accuracy:0.8153590338665659\n",
      "Tue Mar  3 02:06:13 2020\n",
      "Epoch: 5/6---finished---accuracy:0.8031222328271977\n",
      "Tue Mar  3 02:53:38 2020\n",
      "Epoch: 6/6---finished---accuracy:0.8099662223464174\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 6\n",
    "clip = 5 # Gradient clipping\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "net = net.to(device)\n",
    "\n",
    "net.train()\n",
    "# Now our network is in training mode, lets train for some epochs\n",
    "#accuracy_old_old =0.0\n",
    "loss_vs_epoch = []\n",
    "valloss_vs_epoch = []\n",
    "valid_loss_old = 100.0\n",
    "bert_model.to('cuda')\n",
    "bert_bs = train_bs\n",
    "for e in range(0,epochs):\n",
    "    t1=time.ctime()\n",
    "    print(t1)\n",
    "    # create hidden state\n",
    "    # do this for every batch\n",
    "    for q1t, q1id, q1m, q2t, q2id, q2m, labels in train_loader:\n",
    "        with torch.no_grad():\n",
    "            q1_embed = []\n",
    "            q2_embed = []\n",
    "            q1t = q1t.long().to(device)\n",
    "            q1id = q1id.long().to(device)\n",
    "            q1m = q1m.long().to(device)\n",
    "            q2t = q2t.long().to(device)\n",
    "            q2id = q2id.long().to(device)\n",
    "            q2m = q2m.long().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            q1_bert = bert_model(q1t, q1id, q1m)[0]\n",
    "            q1_embed.append(q1_bert)        \n",
    "            q2_bert = bert_model(q2t, q2id, q2m)[0]\n",
    "            q2_embed.append(q2_bert)        \n",
    "            q1_tensor = torch.cat(q1_embed)\n",
    "            q2_tensor = torch.cat(q2_embed)\n",
    "        batch_size=q1t.shape[0]\n",
    "        #print(batch_size)\n",
    "        h1 = net.init_hidden(batch_size)\n",
    "        h2 = net.init_hidden(batch_size)\n",
    "        # Create new variables for the hidden state, so we don't backpropagate through entire training history\n",
    "        h1 = tuple([each.data for each in h1])\n",
    "        h2 = tuple([each.data for each in h2])\n",
    "        # zero out the accumulated gradients\n",
    "        net.zero_grad()\n",
    "        #x1=x1.float()\n",
    "        output, h1, h2 = net(q1_tensor, q2_tensor, h1, h2)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    loss_vs_epoch.append([e+1, loss.item()])\n",
    "    val_losses = []\n",
    "    net.eval()\n",
    "    avg_acc = 0\n",
    "    preds = []\n",
    "    original = []\n",
    "    for q1t, q1id, q1m, q2t, q2id, q2m, labels in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            q1_embed = []\n",
    "            q2_embed = []\n",
    "            q1t = q1t.long().to(device)\n",
    "            q1id = q1id.long().to(device)\n",
    "            q1m = q1m.long().to(device)\n",
    "            q2t = q2t.long().to(device)\n",
    "            q2id = q2id.long().to(device)\n",
    "            q2m = q2m.long().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            q1_bert = bert_model(q1t, q1id, q1m)[0]\n",
    "            q1_embed.append(q1_bert)        \n",
    "            q2_bert = bert_model(q2t, q2id, q2m)[0]\n",
    "            q2_embed.append(q2_bert)        \n",
    "            q1_tensor = torch.cat(q1_embed)\n",
    "            q2_tensor = torch.cat(q2_embed)\n",
    "        batch_size=q1t.shape[0]\n",
    "        #print(batch_size)\n",
    "        val_h1 = net.init_hidden(batch_size)\n",
    "        val_h2 = net.init_hidden(batch_size)\n",
    "        # Create new variables for the hidden state, so we don't backpropagate through entire training history\n",
    "        val_h1 = tuple([each.data for each in val_h1])\n",
    "        val_h2 = tuple([each.data for each in val_h2])\n",
    "        output, val_h1, val_h2 = net(q1_tensor, q2_tensor, val_h1, val_h2)\n",
    "        val_loss = criterion(output, labels)\n",
    "        acc = accuracy(labels.cpu().numpy(), output.detach().cpu().numpy().squeeze())\n",
    "        val_losses.append(val_loss.item())\n",
    "        preds.append(output.cpu().detach().numpy())\n",
    "        original.append(labels.float().cpu().detach().numpy())\n",
    "        avg_acc += acc\n",
    "    net.train()\n",
    "    print( \"Epoch: {}/{}---finished---accuracy:{}\".format(e+1,epochs, avg_acc / len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
